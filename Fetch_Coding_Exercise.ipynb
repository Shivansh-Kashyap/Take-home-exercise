{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***TASK 1***"
      ],
      "metadata": {
        "id": "AfGReN-B5lfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "USING MINILM-L6-V2"
      ],
      "metadata": {
        "id": "09CcRotRouXf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8TWur9APoXAh"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"This is the first excercise.\",\n",
        "    \"I am using MiniLM L6.\",\n",
        "    \"This is extremely fun.\"\n",
        "]\n",
        "\n",
        "\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "\n",
        "def print_embeddings(sentences, embeddings):\n",
        "    for sentence, embedding in zip(sentences, embeddings):\n",
        "        print(f\"Sentence: {sentence}\")\n",
        "        print(f\"Embedding: {embedding[:5]}... (truncated)\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_embeddings(sentences, embeddings)"
      ],
      "metadata": {
        "id": "M8wywO32qMcB",
        "outputId": "4c68cf53-8f40-41c5-e8e5-5a18f2d23464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: This is the first excercise.\n",
            "Embedding: [-0.0258184  -0.05110373  0.0023471   0.07200564 -0.09817741]... (truncated)\n",
            "\n",
            "Sentence: I am using MiniLM L6.\n",
            "Embedding: [ 0.01930069 -0.06063944 -0.07139064 -0.037043    0.03860332]... (truncated)\n",
            "\n",
            "Sentence: This is extremely fun.\n",
            "Embedding: [ 0.03399094 -0.00855662  0.0275319  -0.04598624 -0.06823699]... (truncated)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "USING BERT BASE"
      ],
      "metadata": {
        "id": "WCz8fKLRqAVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"bert-base-nli-mean-tokens\")\n",
        "\n",
        "sentences = [\n",
        "    \"This is the first excercise.\",\n",
        "    \"I am using MiniLM L6.\",\n",
        "    \"This is extremely fun.\"\n",
        "]\n",
        "\n",
        "\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "\n",
        "def print_embeddings(sentences, embeddings):\n",
        "    for sentence, embedding in zip(sentences, embeddings):\n",
        "        print(f\"Sentence: {sentence}\")\n",
        "        print(f\"Embedding: {embedding[:5]}... (truncated)\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vQdnyiV1qA8C"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_embeddings(sentences, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpf9SE0-qPIL",
        "outputId": "a49c01b8-5efc-4bca-b231-739071642ff4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: This is the first excercise.\n",
            "Embedding: [-0.10511372  0.06052209  0.7207366   0.25130892 -0.14731582]... (truncated)\n",
            "\n",
            "Sentence: I am using MiniLM L6.\n",
            "Embedding: [ 0.2196106   0.12576272  1.3329709  -0.03008128  0.50363946]... (truncated)\n",
            "\n",
            "Sentence: This is extremely fun.\n",
            "Embedding: [-0.00433037 -0.466831    2.1145084   0.36574316 -0.09815084]... (truncated)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see we obtained the exact same embeddings from 2 different model implementations from Hugging Face. The purpose behind using BERT is, its a complex model typically used to capture rich contextual embeddings. On the other hand MiniLM-L6 model is a compact model more suitable large scale tasks. It is used to get meaning full embeddings without losing to much depth. I implemented both models to showcase that for a small sample size they both provide similar embeddings so a faster model would be a better fit.\n"
      ],
      "metadata": {
        "id": "a5ZrDe_lqq_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2**"
      ],
      "metadata": {
        "id": "iKllkNqO6AJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiTaskSentenceModel(nn.Module):\n",
        "    def __init__(self, transformer_model=\"paraphrase-MiniLM-L6-v2\", hidden_size=384, num_labels_task_a=3, num_labels_task_b=2):\n",
        "        super(MultiTaskSentenceModel, self).__init__()\n",
        "        self.encoder = SentenceTransformer(transformer_model)\n",
        "        self.task_a_head = nn.Linear(hidden_size, num_labels_task_a)\n",
        "        self.task_b_head = nn.Linear(hidden_size, num_labels_task_b)\n",
        "\n",
        "\n",
        "        self.task_a_labels = [\"Finance\", \"Entertainment\", \"Technology\"]\n",
        "        self.task_b_labels = [\"Negative\", \"Positive\"]\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        embeddings = self.encoder.encode(sentences, convert_to_tensor=True)\n",
        "        logits_a = self.task_a_head(embeddings)\n",
        "        logits_b = self.task_b_head(embeddings)\n",
        "        return logits_a, logits_b\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        logits_a, logits_b = self.forward(sentences)\n",
        "        probs_a = F.softmax(logits_a, dim=1)\n",
        "        probs_b = F.softmax(logits_b, dim=1)\n",
        "        pred_a = torch.argmax(probs_a, dim=1)\n",
        "        pred_b = torch.argmax(probs_b, dim=1)\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            print(f\"Sentence: {sentence}\")\n",
        "            print(f\"\\tPredicted Topic: {self.task_a_labels[pred_a[i]]}\")\n",
        "            print(f\"\\tPredicted Sentiment: {self.task_b_labels[pred_b[i]]}\\n\")\n",
        "\n",
        "\n",
        "model = MultiTaskSentenceModel()\n",
        "sample_sentences = [\n",
        "    \"This is the second excercise.\",\n",
        "    \"I am using MiniLM L6.\",\n",
        "    \"This is extremely fun.\"\n",
        "]\n",
        "\n",
        "model.predict(sample_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA_wXZvSPd_l",
        "outputId": "ae9bec37-eef6-4954-f0ff-fbdd0eeec718"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: This is the second excercise.\n",
            "\tPredicted Topic: Entertainment\n",
            "\tPredicted Sentiment: Positive\n",
            "\n",
            "Sentence: I am using MiniLM L6.\n",
            "\tPredicted Topic: Entertainment\n",
            "\tPredicted Sentiment: Negative\n",
            "\n",
            "Sentence: This is extremely fun.\n",
            "\tPredicted Topic: Finance\n",
            "\tPredicted Sentiment: Positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this task i have decided to implement sentence classification with a sentiment analysis classification. To achieve this i have implemented two task specific heads. These heads have been implemented with a linear neural network. The end result being we have a very basic sentence/sentiment analysis model."
      ],
      "metadata": {
        "id": "iDau_jb7ZVsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 3**"
      ],
      "metadata": {
        "id": "gFYoKaunaeqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.) If the entire network is frozen, including both the transformer backbone and task-specific heads, the model can only act as a static feature extractor. This makes it so the mdeol cannot adapt to new tasks, and the task heads will produce essentially random outputs due to untrained weights."
      ],
      "metadata": {
        "id": "jNWwHxxfgxtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.)If we are to freeze only the transformer backbone while training the task-specific heads then this works really well when we are working with limited labeled data, as it prevents overfitting and allows the model to learn meaningful decision boundaries using the embeddings generated by the MiniLM model."
      ],
      "metadata": {
        "id": "TEvC2YfGhtlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.)In case of only one of the task head being frozen for ex- keeping the topic classification head static while continuing to train the sentiment classification head, this would help in preserving performance on well-functioning tasks while allowing fine-tuning of underperforming components without disrupting the entire system."
      ],
      "metadata": {
        "id": "8ARYjXk1iPtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenarios"
      ],
      "metadata": {
        "id": "ZTOrhysUiitt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.)When transfer learning is viable, the process should begin with choosing a powerful and efficient pre-trained transformer model, such as parMiniLM-L6-v2, which is optimized for sentence-level semantic understanding."
      ],
      "metadata": {
        "id": "GsoFhwtsio_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.)I would choose to freeze the transformer and only train the added task heads to quickly adapt to the target task without modifying the core representations. This makes it as training progresses and more labeled data becomes available, gradually unfreezing the upper layers of the transformer allows the model to become more task-aware, enabling better adaptation to domain-specific nuances."
      ],
      "metadata": {
        "id": "y58yDVWclfvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.) This fine-tuning approach ensures a balance between leveraging general language knowledge and learning task-specific patterns. Ultimately, freezing and unfreezing strategies should be chosen based on the amount of available data, the complexity of tasks, and the desired balance between generalization and specialization.\n",
        "\n"
      ],
      "metadata": {
        "id": "tGjPXvCJl0d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 4**"
      ],
      "metadata": {
        "id": "1rBe8eqMpDvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "class MultiTaskSentenceModel(nn.Module):\n",
        "    def __init__(self, transformer_model=\"paraphrase-MiniLM-L6-v2\", hidden_size=384, num_labels_task_a=3, num_labels_task_b=2):\n",
        "        super(MultiTaskSentenceModel, self).__init__()\n",
        "        self.encoder = SentenceTransformer(transformer_model)\n",
        "        self.task_a_head = nn.Linear(hidden_size, num_labels_task_a)\n",
        "        self.task_b_head = nn.Linear(hidden_size, num_labels_task_b)\n",
        "        self.task_a_labels = [\"Finance\", \"Entertainment\", \"Technology\"]\n",
        "        self.task_b_labels = [\"Negative\", \"Positive\"]\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        embeddings = self.encoder.encode(sentences, convert_to_tensor=True)\n",
        "        logits_a = self.task_a_head(embeddings)\n",
        "        logits_b = self.task_b_head(embeddings)\n",
        "        return logits_a, logits_b\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        logits_a, logits_b = self.forward(sentences)\n",
        "        probs_a = F.softmax(logits_a, dim=1)\n",
        "        probs_b = F.softmax(logits_b, dim=1)\n",
        "        pred_a = torch.argmax(probs_a, dim=1)\n",
        "        pred_b = torch.argmax(probs_b, dim=1)\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            print(f\"Sentence: {sentence}\")\n",
        "            print(f\"\\tPredicted Topic: {self.task_a_labels[pred_a[i]]}\")\n",
        "            print(f\"\\tPredicted Sentiment: {self.task_b_labels[pred_b[i]]}\\n\")\n",
        "\n",
        "\n",
        "model = MultiTaskSentenceModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"The stock market crashed due to inflation.\",\n",
        "    \"The film was a cinematic masterpiece.\",\n",
        "    \"Tech companies are investing in AI.\",\n",
        "    \"The weather ruined my mood today.\",\n",
        "    \"Quantum computing is the next big thing.\"\n",
        "]\n",
        "\n",
        "task_a_labels = torch.tensor([0, 1, 2, 1, 2])\n",
        "task_b_labels = torch.tensor([0, 1, 1, 0, 1])\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    logits_a, logits_b = model(sentences)\n",
        "    loss_a = loss_fn(logits_a, task_a_labels)\n",
        "    loss_b = loss_fn(logits_b, task_b_labels)\n",
        "    total_loss = loss_a + loss_b\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    pred_a = torch.argmax(logits_a, dim=1)\n",
        "    pred_b = torch.argmax(logits_b, dim=1)\n",
        "    acc_a = (pred_a == task_a_labels).float().mean().item()\n",
        "    acc_b = (pred_b == task_b_labels).float().mean().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss.item():.4f} | Task A Acc: {acc_a:.2f} | Task B Acc: {acc_b:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwbT5yXqpIMp",
        "outputId": "2e776484-516b-4eff-872c-07fbb05b44c5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 1.7453 | Task A Acc: 0.40 | Task B Acc: 0.20\n",
            "Epoch 2 | Loss: 1.7428 | Task A Acc: 0.40 | Task B Acc: 0.20\n",
            "Epoch 3 | Loss: 1.7402 | Task A Acc: 0.40 | Task B Acc: 0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Task 2, the focus was solely on building the multi-task learning architecture by introducing a shared transformer encoder and two task-specific classification heads—one for topic classification and another for sentiment analysis. However, Task 4 expanded on this by implementing the training mechanics necessary to optimize the model. Specifically, synthetic sentences and randomly assigned labels were introduced to simulate a dataset. Two separate loss functions using CrossEntropyLoss were applied—one for each task—and an Adam optimizer was initialized to update the model’s parameters. A training loop was constructed to perform multiple epochs of training, including the forward pass, loss computation for both tasks, backpropagation, and parameter updates. Also after each epoch the loop calculated and printed accuracy metrics for both task heads, offering insight into learning progress. These additions transformed the static architecture from Task 2 into a functional training pipeline capable of handling multi-task optimization, even with hypothetical data.\n",
        "\n"
      ],
      "metadata": {
        "id": "NbPSQk1QphZh"
      }
    }
  ]
}